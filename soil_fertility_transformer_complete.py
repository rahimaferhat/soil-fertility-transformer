# -*- coding: utf-8 -*-
"""Soil_Fertility_Transformer_Complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...
"""

import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import math

# Try to mount drive, but handle failure gracefully (for local testing)
try:
    from google.colab import drive
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

# =============================================================================
# 1. CONFIGURATION
# =============================================================================
print("="*80)
print("‚öôÔ∏è CONFIGURATION & SETUP")
print("="*80)

if IN_COLAB:
    print("‚òÅÔ∏è Running in Google Colab")
    try:
        drive.mount('/content/drive')
        # USER: CHANGE THIS PATH TO YOUR ACTUAL DRIVE FOLDER IF DIFFERENT
        DRIVE_PATH = '/content/drive/MyDrive/files' 
    except Exception as e:
        print(f"‚ö†Ô∏è Drive mount failed: {e}")
        DRIVE_PATH = './content/drive/MyDrive/files'
else:
    print("üíª Running Locally")
    DRIVE_PATH = './soil_fertility_project'

# Model & Artifacts Storage
MODEL_SAVE_DIR = os.path.join(DRIVE_PATH, 'models')
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# Hyperparameters (Optimized for PERFORMANCE)
SEQUENCE_LENGTH = 1000  # Number of spectral bands
N_FEATURES = 10         # Number of features per band (e.g., after PCA)
BATCH_SIZE = 32
EPOCHS = 100            # Increased for better convergence
LEARNING_RATE = 1e-3

print(f"üìÇ Working Directory: {DRIVE_PATH}")
print(f"üíæ Model Save Path:   {MODEL_SAVE_DIR}")
print(f"‚ö° Batch Size:       {BATCH_SIZE}")
print(f"üîÑ Epochs:           {EPOCHS}")

# =============================================================================
# 2. CUSTOM LAYERS (SERIALIZABLE)
# =============================================================================

@keras.utils.register_keras_serializable()
class PositionalEncoding(layers.Layer):
    """
    Positional encoding for spectral sequence position.
    Registered as serializable to ensure model saving/loading works correctly.
    """
    def __init__(self, sequence_length, d_model, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.sequence_length = sequence_length
        self.d_model = d_model
        # We build the encoding matrix in __init__ so it's ready
        self.pos_encoding = self._get_positional_encoding(sequence_length, d_model)

    def _get_positional_encoding(self, length, depth):
        depth = depth / 2
        positions = np.arange(length)[:, np.newaxis]
        depths = np.arange(depth)[np.newaxis, :] / depth
        angle_rates = 1 / (10000**depths)
        angle_rads = positions * angle_rates
        pos_encoding = np.concatenate(
            [np.sin(angle_rads), np.cos(angle_rads)],
            axis=-1)
        return tf.cast(pos_encoding, dtype=tf.float32)

    def get_config(self):
        config = super(PositionalEncoding, self).get_config()
        config.update({
            "sequence_length": self.sequence_length,
            "d_model": self.d_model,
        })
        return config

    def call(self, x):
        # Add encoding to input (broadcast along batch dimension)
        return x + self.pos_encoding[tf.newaxis, :tf.shape(x)[1], :]

def transformer_encoder_block(inputs, head_size, num_heads, ff_dim, dropout=0):
    """
    Standard Transformer Encoder Block
    """
    # Multi-Head Attention
    attention_output = layers.MultiHeadAttention(
        key_dim=head_size,
        num_heads=num_heads,
        dropout=dropout
    )(inputs, inputs)
    attention_output = layers.Dropout(dropout)(attention_output)

    # Add & Norm
    x = layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)

    # Feed-Forward Network
    ffn_output = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    ffn_output = layers.Dropout(dropout)(ffn_output)
    ffn_output = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ffn_output)

    # Add & Norm
    output = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)
    return output

def build_hyperspectral_transformer(
    input_shape,
    d_model=64,             # INCREASED: More capacity
    num_heads=4,            # INCREASED: Focus on different parts of spectrum
    ff_dim=256,             # INCREASED: Wider internal layers
    num_transformer_blocks=4, # DOUBLED: Deeper network
    mlp_units=[128, 64],    # DEEPER: Stronger regressor head
    dropout=0.1,
    mlp_dropout=0.2,
):
    """
    Builds the complete model architecture.
    """
    inputs = keras.Input(shape=input_shape)

    # Project input to d_model dimensions
    x = layers.Dense(d_model, name="input_projection")(inputs)

    # Add Positional Encoding
    x = PositionalEncoding(input_shape[0], d_model, name="positional_encoding")(x)
    x = layers.Dropout(dropout)(x)

    # Transformer Blocks
    for i in range(num_transformer_blocks):
        x = transformer_encoder_block(
            x,
            head_size=d_model // num_heads,
            num_heads=num_heads,
            ff_dim=ff_dim,
            dropout=dropout
        )

    # Global Pooling (Flattening sequence)
    x = layers.GlobalAveragePooling1D(name="global_pooling")(x)

    # MLP Head for Regression
    for i, dim in enumerate(mlp_units):
        x = layers.Dense(dim, activation="relu", name=f"mlp_dense_{i}")(x)
        x = layers.Dropout(mlp_dropout)(x)

    # Single output for Fertility Index (Regression)
    outputs = layers.Dense(1, activation="linear", name="fertility_output")(x)

    model = keras.Model(inputs, outputs, name="HyperspectralTransformer")
    return model

# =============================================================================
# 3. DATA GENERATION & LOADING
# =============================================================================

def create_synthetic_data(n_samples=1000):
    """
    Creates realistic-looking synthetic hyperspectral data.
    """
    print(f"üß™ Generating {n_samples} samples of synthetic data...")
    np.random.seed(42)
    
    X = np.zeros((n_samples, SEQUENCE_LENGTH, N_FEATURES))
    Y = np.zeros((n_samples, 1))

    for i in range(n_samples):
        # Generate base signal (spectral curve simulation)
        t = np.linspace(0, 10, SEQUENCE_LENGTH)
        base = np.sin(t * np.random.uniform(0.5, 1.5)) * np.exp(-t/10)
        
        for j in range(N_FEATURES):
            # Add feature-specific noise/variation
            noise = np.random.normal(0, 0.1, SEQUENCE_LENGTH)
            X[i, :, j] = base + noise + np.random.normal(0, 0.5)

        # Target (Fertility) depends on spectral characteristics
        # Simple heuristic: mean of first few bands - variance of last few bands
        signal_mean = np.mean(X[i, :100, :])
        signal_var = np.var(X[i, -100:, :])
        fertility = 0.5 * signal_mean - 0.2 * signal_var + 0.5
        
        # Add random noise to target
        Y[i] = fertility + np.random.normal(0, 0.05)

    return X.astype(np.float32), Y.astype(np.float32)

def get_data():
    """
    Attempts to load real data, falls back to synthetic.
    """
    try:
        # Define paths to your supposed real data
        x_path = os.path.join(DRIVE_PATH, 'X_train.npy')
        y_path = os.path.join(DRIVE_PATH, 'Y_train.npy')
        
        if os.path.exists(x_path) and os.path.exists(y_path):
            print(f"üìÇ Loading real data from {DRIVE_PATH}...")
            X = np.load(x_path)
            Y = np.load(y_path)
            # Split manually for simplicity here
            split = int(len(X) * 0.8)
            return (X[:split], Y[:split]), (X[split:], Y[split:])
        else:
            raise FileNotFoundError("Real data files not found.")
            
    except Exception as e:
        print(f"‚ö†Ô∏è {e}")
        print("‚ö†Ô∏è Falling back to synthetic data creation.")
        X, Y = create_synthetic_data(n_samples=1500)
        split = int(len(X) * 0.8)
        return (X[:split], Y[:split]), (X[split:], Y[split:])

# =============================================================================
# 4. MAIN EXECUTION
# =============================================================================

# A. Load & Preprocess Data
print("\n" + "="*80)
print("üìä DATA LOADING")
print("="*80)
(X_train, Y_train), (X_val, Y_val) = get_data()

print("üîß Preprocessing (Scaling)...")
scaler_X = StandardScaler()
# Reshape for scaling: (Samples * Time, Features)
X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])

X_train_scaled = scaler_X.fit_transform(X_train_reshaped).reshape(X_train.shape)
X_val_scaled = scaler_X.transform(X_val_reshaped).reshape(X_val.shape)

scaler_Y = StandardScaler()
Y_train_scaled = scaler_Y.fit_transform(Y_train)
Y_val_scaled = scaler_Y.transform(Y_val)
print("‚úÖ Data ready.")

# B. Build & Train Model
print("\n" + "="*80)
print("üèóÔ∏è MODEL TRAINING")
print("="*80)

model = build_hyperspectral_transformer(input_shape=(SEQUENCE_LENGTH, N_FEATURES))
model.summary()

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=1.0),
    loss="huber",
    metrics=["mae"]
)

callbacks = [
    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
]

print("üöÄ Starting Training...")
history = model.fit(
    X_train_scaled, Y_train_scaled,
    validation_data=(X_val_scaled, Y_val_scaled),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

# C. Evaluation
print("\n" + "="*80)
print("üìà EVALUATION")
print("="*80)

Y_pred_scaled = model.predict(X_val_scaled, verbose=0)

mae = mean_absolute_error(Y_val, Y_pred)
r2 = r2_score(Y_val, Y_pred)

print(f"‚úÖ Validation MAE: {mae:.4f}")
print(f"‚úÖ Validation R¬≤:  {r2:.4f}")

# Visualization
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(Y_val, Y_pred, alpha=0.5, c='green')
plt.plot([Y_val.min(), Y_val.max()], [Y_val.min(), Y_val.max()], 'r--', lw=2)
plt.title(f'Actual vs Predicted (R¬≤={r2:.2f})')
plt.xlabel('Actual Fertility')
plt.ylabel('Predicted Fertility')
plt.grid(True, alpha=0.3)

plt.tight_layout()
viz_path = os.path.join(MODEL_SAVE_DIR, 'results_plot.png')
plt.savefig(viz_path)
print(f"üìä Plot saved to: {viz_path}")
plt.show()

# Save Quantitative Report
report_path = os.path.join(MODEL_SAVE_DIR, 'evaluation_metrics.txt')
rmse = np.sqrt(mean_squared_error(Y_val, Y_pred))

with open(report_path, 'w') as f:
    f.write("SOIL FERTILITY PREDICTION - QUANTITATIVE RESULTS\n")
    f.write("================================================\n")
    f.write(f"Model Architecture: Transformer (d_model={model.layers[1].output.shape[-1]})\n")
    f.write(f"Test Set Size:      {len(Y_val)} samples\n\n")
    f.write("EVALUATION METRICS:\n")
    f.write(f"-------------------\n")
    f.write(f"R¬≤ Score (Accuracy): {r2:.4f} ({r2*100:.2f}%)\n")
    f.write(f"MAE (Mean Abs Error): {mae:.4f}\n")
    f.write(f"RMSE (Rt Mean Sq Err):{rmse:.4f}\n")
    f.write("\nINTERPRETATION:\n")
    if r2 > 0.75:
        f.write("- The model shows strong predictive performance (R¬≤ > 0.75).\n")
    elif r2 > 0.5:
        f.write("- The model shows moderate predictive performance.\n")
    else:
        f.write("- The model performance suggests further optimization is needed.\n")

print(f"üìÑ Quantitative Report saved to: {report_path}")

# =============================================================================
# 5. SAVING & VERIFICATION (CRITICAL STEP)
# =============================================================================
print("\n" + "="*80)
print("üíæ SAVING & VERIFICATION")
print("="*80)

# 1. Save Model
model_path = os.path.join(MODEL_SAVE_DIR, 'soil_fertility_transformer.keras')
print(f"... Saving model to {model_path}")
model.save(model_path)

# 2. Save Scalers
print("... Saving scalers")
with open(os.path.join(MODEL_SAVE_DIR, 'scalers.pkl'), 'wb') as f:
    pickle.dump({'scaler_X': scaler_X, 'scaler_Y': scaler_Y}, f)

# 3. VERIFY LOADING (The Test)
print("... üßê Verification: Attempting to reload model immediately...")
try:
    # IMPORTANT: Since we used @register_keras_serializable(), we usually 
    # don't need custom_objects, but it's safe to load simple.
    loaded_model = keras.models.load_model(model_path)
    
    # Test Prediction
    test_sample = X_val_scaled[:5]
    orig_pred = model.predict(test_sample, verbose=0)
    load_pred = loaded_model.predict(test_sample, verbose=0)
    
    diff = np.mean(np.abs(orig_pred - load_pred))
    print(f"   Original Preds (first 2): {orig_pred[:2].flatten()}")
    print(f"   Loaded Preds   (first 2): {load_pred[:2].flatten()}")
    print(f"   Difference: {diff:.6f}")
    
    if diff < 1e-6:
        print("‚úÖ SUCCESS: Model saved and reloaded perfectly! Your file is safe.")
    else:
        print("‚ö†Ô∏è WARNING: Predictions differ slightly (check precision).")
        
except Exception as e:
    print(f"‚ùå ERROR: Failed to reload the model.\n{e}")

print("\nüéâ DONE! You can now download your model from:", model_path)
